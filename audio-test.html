<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio Transcription Test</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .controls {
            text-align: center;
            margin: 30px 0;
        }
        button {
            font-size: 18px;
            padding: 15px 30px;
            margin: 10px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        .record-btn {
            background-color: #4CAF50;
            color: white;
        }
        .record-btn:hover {
            background-color: #45a049;
        }
        .record-btn:disabled {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        .recording {
            background-color: #f44336 !important;
        }
        .status {
            text-align: center;
            margin: 20px 0;
            font-weight: bold;
            font-size: 16px;
        }
        .log {
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            padding: 15px;
            border-radius: 5px;
            max-height: 400px;
            overflow-y: auto;
            font-family: monospace;
            font-size: 12px;
            margin-top: 20px;
        }
        .transcript {
            background-color: #e8f5e8;
            border: 2px solid #4CAF50;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            min-height: 50px;
        }
        .error {
            background-color: #ffe8e8;
            border: 2px solid #f44336;
            color: #d32f2f;
        }
        .success {
            color: #2e7d32;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üé§ OpenAI Audio Transcription Test</h1>
        
        <div class="controls">
            <button id="recordBtn" class="record-btn">üé§ Hold to Record</button>
            <button id="clearBtn">üóëÔ∏è Clear Log</button>
        </div>
        
        <div id="status" class="status">Ready to record</div>
        
        <div class="transcript" id="transcript">
            <strong>Transcript will appear here...</strong>
        </div>
        
        <div class="log" id="log"></div>
    </div>

    <script>
        class SimpleAudioTest {
            constructor() {
                this.ws = null;
                this.audioContext = null;
                this.mediaStream = null;
                this.audioProcessor = null;
                this.isRecording = false;
                this.audioBuffer = [];
                this.isConnected = false;
                
                this.recordBtn = document.getElementById('recordBtn');
                this.clearBtn = document.getElementById('clearBtn');
                this.status = document.getElementById('status');
                this.transcript = document.getElementById('transcript');
                this.log = document.getElementById('log');
                
                this.setupEventListeners();
                this.connect();
            }
            
            log(message, type = 'info') {
                const timestamp = new Date().toLocaleTimeString();
                const color = type === 'error' ? '#d32f2f' : type === 'success' ? '#2e7d32' : '#333';
                this.log.innerHTML += `<div style="color: ${color};">[${timestamp}] ${message}</div>`;
                this.log.scrollTop = this.log.scrollHeight;
                console.log(message);
            }
            
            updateStatus(message, type = 'info') {
                this.status.textContent = message;
                this.status.className = `status ${type}`;
            }
            
            setupEventListeners() {
                this.recordBtn.addEventListener('mousedown', () => this.startRecording());
                this.recordBtn.addEventListener('mouseup', () => this.stopRecording());
                this.recordBtn.addEventListener('mouseleave', () => this.stopRecording());
                
                // Also support spacebar
                document.addEventListener('keydown', (e) => {
                    if (e.code === 'Space' && !this.isRecording) {
                        e.preventDefault();
                        this.startRecording();
                    }
                });
                
                document.addEventListener('keyup', (e) => {
                    if (e.code === 'Space') {
                        e.preventDefault();
                        this.stopRecording();
                    }
                });
                
                this.clearBtn.addEventListener('click', () => {
                    this.log.innerHTML = '';
                    this.transcript.innerHTML = '<strong>Transcript will appear here...</strong>';
                });
            }
            
            async connect() {
                try {
                    this.updateStatus('Connecting to OpenAI...', 'info');
                    this.log('üîå Connecting to OpenAI Realtime API...');
                    
                    // Connect to backend proxy (adjust URL for your deployment)
                    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                    const backendHost = window.location.hostname === 'localhost' ? 'localhost:3001' : 'polycast-server.onrender.com';
                    const url = `${protocol}//${backendHost}/openai-proxy`;
                    
                    this.ws = new WebSocket(url);
                    
                    this.ws.onopen = () => {
                        this.log('‚úÖ Connected to OpenAI proxy', 'success');
                        this.isConnected = true;
                        this.updateStatus('Connected - ready to test audio', 'success');
                    };
                    
                    this.ws.onmessage = (event) => {
                        try {
                            const message = JSON.parse(event.data);
                            this.handleMessage(message);
                        } catch (error) {
                            this.log('‚ùå Error parsing message: ' + error.message, 'error');
                        }
                    };
                    
                    this.ws.onclose = () => {
                        this.log('üîå Connection closed', 'error');
                        this.isConnected = false;
                        this.updateStatus('Disconnected', 'error');
                    };
                    
                    this.ws.onerror = (error) => {
                        this.log('‚ùå WebSocket error: ' + error, 'error');
                        this.updateStatus('Connection error', 'error');
                    };
                    
                } catch (error) {
                    this.log('‚ùå Failed to connect: ' + error.message, 'error');
                    this.updateStatus('Connection failed', 'error');
                }
            }
            
            handleMessage(message) {
                const eventType = message.type;
                
                switch (eventType) {
                    case 'session.created':
                        this.log('‚úÖ Session created, configuring...', 'success');
                        this.sendConfiguration();
                        break;
                        
                    case 'session.updated':
                        this.log('‚úÖ Session configured successfully', 'success');
                        this.initializeAudio();
                        break;
                        
                    case 'input_audio_buffer.cleared':
                        this.log('üóëÔ∏è Audio buffer cleared');
                        break;
                        
                    case 'input_audio_buffer.committed':
                        this.log('‚úÖ Audio buffer committed', 'success');
                        break;
                        
                    case 'conversation.item.created':
                        this.log('üìù Conversation item created');
                        break;
                        
                    case 'conversation.item.input_audio_transcription.completed':
                        this.log('‚úÖ Transcription completed!', 'success');
                        this.transcript.innerHTML = `<strong>‚úÖ SUCCESS:</strong> "${message.transcript}"`;
                        this.transcript.className = 'transcript success';
                        this.updateStatus('Transcription successful!', 'success');
                        break;
                        
                    case 'conversation.item.input_audio_transcription.failed':
                        this.log('‚ùå Transcription failed: ' + JSON.stringify(message.error), 'error');
                        this.transcript.innerHTML = `<strong>‚ùå FAILED:</strong> ${JSON.stringify(message.error)}`;
                        this.transcript.className = 'transcript error';
                        this.updateStatus('Transcription failed', 'error');
                        break;
                        
                    default:
                        if (!eventType.includes('delta') && !eventType.includes('transcript')) {
                            this.log('üì® Received: ' + eventType);
                        }
                        break;
                }
            }
            
            sendConfiguration() {
                const config = {
                    type: 'session.update',
                    session: {
                        modalities: ['audio', 'text'],
                        instructions: 'You are a helpful assistant. Just acknowledge what the user says.',
                        voice: 'alloy',
                        input_audio_format: 'pcm16',
                        output_audio_format: 'pcm16',
                        turn_detection: null,
                        input_audio_transcription: {
                            model: 'whisper-1'
                        }
                    }
                };
                
                this.log('üì§ Sending session configuration...');
                this.ws.send(JSON.stringify(config));
            }
            
            async initializeAudio() {
                try {
                    this.log('üé§ Initializing audio (NEW: Backend Processing)...');
                    
                    // Simple audio setup - let backend handle all processing
                    const audioConstraints = {
                        sampleRate: 48000, // Use whatever the device provides
                        channelCount: 1,
                        echoCancellation: true,
                        noiseSuppression: false,
                        autoGainControl: false
                    };
                    
                    this.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: audioConstraints });
                    
                    // Get actual settings
                    const audioTrack = this.mediaStream.getAudioTracks()[0];
                    const settings = audioTrack.getSettings();
                    this.actualSampleRate = settings.sampleRate || 48000;
                    
                    this.log(`üîß MediaStream: ${settings.sampleRate}Hz, ${settings.channelCount}ch`);
                    
                    // Create AudioContext with whatever sample rate we get
                    this.audioContext = new AudioContext({ sampleRate: this.actualSampleRate });
                    
                    if (this.audioContext.state === 'suspended') {
                        await this.audioContext.resume();
                    }
                    
                    this.log(`üîß AudioContext: ${this.audioContext.sampleRate}Hz, state: ${this.audioContext.state}`);
                    
                    const source = this.audioContext.createMediaStreamSource(this.mediaStream);
                    
                    // Set up audio processor - much simpler now!
                    const bufferSize = 4096;
                    this.audioProcessor = this.audioContext.createScriptProcessor(bufferSize, 1, 1);
                    
                    this.audioProcessor.onaudioprocess = (event) => {
                        if (this.isRecording) {
                            const inputData = event.inputBuffer.getChannelData(0);
                            
                            // Simple audio detection
                            let hasAudio = false;
                            for (let i = 0; i < inputData.length; i++) {
                                if (Math.abs(inputData[i]) > 0.005) {
                                    hasAudio = true;
                                    break;
                                }
                            }
                            
                            if (hasAudio) {
                                // NEW: Just send raw Float32 data to backend!
                                this.audioBuffer.push(Array.from(inputData));
                                
                                if (this.audioBuffer.length === 1) {
                                    this.log(`üé§ Capturing raw audio (${this.actualSampleRate}Hz) - backend will process`);
                                }
                            }
                        }
                    };
                    
                    source.connect(this.audioProcessor);
                    this.audioProcessor.connect(this.audioContext.destination);
                    
                    this.log('‚úÖ Audio initialized - backend will handle processing', 'success');
                    this.updateStatus('Ready to record - backend processing mode', 'success');
                    
                } catch (error) {
                    this.log('‚ùå Audio initialization failed: ' + error.message, 'error');
                    this.updateStatus('Audio setup failed', 'error');
                }
            }
            
            startRecording() {
                if (!this.isConnected || this.isRecording) return;
                
                this.log('üé§ Starting recording (NEW: Raw Audio Mode)...');
                this.updateStatus('üé§ Recording... (release to send)', 'info');
                this.recordBtn.textContent = 'üî¥ Recording...';
                this.recordBtn.classList.add('recording');
                
                // NEW: Clear raw audio buffer
                this.ws.send(JSON.stringify({ type: 'raw_audio_clear' }));
                this.audioBuffer = [];
                this.isRecording = true;
            }
            
            stopRecording() {
                if (!this.isRecording) return;
                
                this.log('üé§ Stopping recording...');
                this.updateStatus('Processing audio on backend...', 'info');
                this.recordBtn.textContent = 'üé§ Hold to Record';
                this.recordBtn.classList.remove('recording');
                this.isRecording = false;
                
                if (this.audioBuffer.length > 0) {
                    this.log(`üì§ Sending ${this.audioBuffer.length} raw audio chunks to backend...`);
                    
                    // Calculate duration from raw samples
                    let totalSamples = 0;
                    this.audioBuffer.forEach(chunk => totalSamples += chunk.length);
                    const durationMs = (totalSamples / this.actualSampleRate * 1000).toFixed(2);
                    this.log(`üîß Raw Audio: ${totalSamples} samples, ${durationMs}ms duration at ${this.actualSampleRate}Hz`);
                    
                    // NEW: Send raw audio chunks to backend
                    this.audioBuffer.forEach(audioData => {
                        this.ws.send(JSON.stringify({
                            type: 'raw_audio_chunk',
                            audioData: audioData, // Raw Float32 array
                            sampleRate: this.actualSampleRate
                        }));
                    });
                    
                    // NEW: Commit raw audio (backend will process and send to OpenAI)
                    this.ws.send(JSON.stringify({ type: 'raw_audio_commit' }));
                    
                    // Create response
                    this.ws.send(JSON.stringify({ type: 'response.create' }));
                    
                } else {
                    this.log('‚ö†Ô∏è No audio captured', 'error');
                    this.updateStatus('No audio detected', 'error');
                }
            }
            
            // OLD AUDIO PROCESSING FUNCTIONS REMOVED - BACKEND HANDLES EVERYTHING NOW!
        }
        
        // Start the test when page loads
        window.addEventListener('load', () => {
            new SimpleAudioTest();
        });
    </script>
</body>
</html>